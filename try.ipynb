{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels: int,\n",
    "                 n_heads: int,\n",
    "                 in_proj_bias: bool = False,\n",
    "                 out_proj_bias: bool = True):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.qkv = nn.Linear(channels, channels * 3, bias=in_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = channels // n_heads\n",
    "        self.proj = nn.Linear(channels, channels, bias=out_proj_bias)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        bs, sequence_length, _ = x.shape\n",
    "        \n",
    "        # Compute q, k, v\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        # (bs, n_head, sequence_length,  d_head)\n",
    "        q, k, v = map(lambda w: w.view(bs, sequence_length, self.n_heads, self.d_head).transpose(1, 2), qkv)\n",
    "\n",
    "        # (bs, n_head, sl, d_head) @ (bs, n_head, d_head, sl) => (bs, n_head, sl, sl)\n",
    "        attn_weights = q @ k.transpose(-1, -2) \n",
    "\n",
    "        if mask:\n",
    "            causal_mask = torch.triu(torch.ones_like(attn_weights, dtype=torch.bool), diagonal=1)\n",
    "            attn_weights = attn_weights.masked_fill(causal_mask, -torch.inf)  \n",
    "\n",
    "        attn_weights = torch.sqrt(self.d_head) * attn_weights\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1) # bs, n_head, sl\n",
    "\n",
    "        # (bs, n_head, sl, sl) @ (bs, n_head, sl,  d_head) => (bs, n_head, d_head)\n",
    "        out = attn_weights @ v \n",
    "        out = out.transpose(1, 2).contiguous().view(bs, sequence_length, -1)  # (bs, sequence_length, channels)\n",
    "\n",
    "        out = self.proj(out)  # Final projection\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAttentionBlock(nn.Module):\n",
    "    def __init__(self, groups, sequence_length):\n",
    "        super(VAttentionBlock, self).__init__()\n",
    "        self.group_norm = nn.GroupNorm(groups, sequence_length)\n",
    "        self.attention = SelfAttention(sequence_length, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.view(n, c, -1) # (bs, channels, height * width)\n",
    "        x = x.transpose(-1, -2) # (bs, height * width, channels)\n",
    "        x = self.group_norm(x) # (bs, height * width, channels)\n",
    "        x = self.attention(x) # (bs, height * width, channels)\n",
    "        x = x.transpose(-1, -2).view(n, c, h, w) # (bs, channels, height, width)\n",
    "        x = x + residue\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "channels = 8\n",
    "image_size = 32\n",
    "n_heads = 4\n",
    "d_head = channels // n_heads\n",
    "\n",
    "self_group_norm = nn.GroupNorm(32, image_size * image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 32, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(bs, channels, image_size, image_size)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(bs, channels, -1).transpose(-1, -2) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = self_group_norm(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , sl, _ = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_qkv = nn.Linear(channels, channels * 3, bias=True)\n",
    "qkv = self_qkv(x).chunk(3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = map(lambda w: w.view(bs, sl, n_heads, d_head).transpose(1, 2), qkv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1024, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1024, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1024, 1024])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = q @ k.transpose(-1, -2) \n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1024, 1024])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = torch.triu(torch.ones_like(attn_weights, dtype=torch.bool), diagonal=1)\n",
    "attn_weights = attn_weights.masked_fill(causal_mask, -torch.inf) \n",
    "attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1024, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attn_weights @ v \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 8])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.transpose(1, 2).contiguous().view(bs, sl, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
